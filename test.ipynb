{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb14fba-a3ed-4851-8d1b-03a3cc2427ef",
   "metadata": {},
   "source": [
    "## load llm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ac66a3-2c7a-41d7-b025-e0f2535ad743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58384edcff9a461db02c449214e809f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Inference for FastChat models.\"\"\"\n",
    "import abc\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Iterable, Optional, Dict\n",
    "import warnings\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    LlamaTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    AutoModel,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    T5Tokenizer,\n",
    "    AutoConfig,\n",
    ")\n",
    "from transformers.generation.logits_process import (\n",
    "    LogitsProcessorList,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "\n",
    "# from fastchat.conversation import get_conv_template, SeparatorStyle\n",
    "# from fastchat.model.model_adapter import (\n",
    "#     load_model,\n",
    "#     get_conversation_template,\n",
    "#     get_generate_stream_function,\n",
    "# )\n",
    "# from fastchat.modules.gptq import GptqConfig\n",
    "# from fastchat.modules.awq import AWQConfig\n",
    "# from fastchat.utils import is_partial_stop, is_sentence_complete, get_context_length\n",
    "\n",
    "# 忽略指定的警告信息\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*TypedStorage is deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*do_sample.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*temperature.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*top_p.*\")\n",
    "\n",
    "model_path = '/home/ai/Code/FastChat/lmsys/vicuna-7b-v1.5'\n",
    "# num_gpus=2\n",
    "load_8bit = False\n",
    "torch.cuda.set_device(0)\n",
    "device = 'cuda'\n",
    "cpu_offloading = False\n",
    "# gptq_config = GptqConfig(ckpt='/home/ai/Code/FastChat/lmsys/vicuna-7b-v1.5', wbits=16, groupsize=-1, act_order=False)\n",
    "# awq_config = AWQConfig(ckpt='/home/ai/Code/FastChat/lmsys/vicuna-7b-v1.5', wbits=16, groupsize=-1)\n",
    "\n",
    "# model, tokenizer = load_model(\n",
    "#         model_path,\n",
    "#         device=device,\n",
    "#         num_gpus=num_gpus,\n",
    "#         max_gpu_memory='7GiB',\n",
    "#         load_8bit=load_8bit,\n",
    "#         cpu_offloading=cpu_offloading,\n",
    "#         gptq_config=gptq_config,\n",
    "#         awq_config=awq_config,\n",
    "#         revision='main',\n",
    "#         debug=True,\n",
    "# )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path, use_fast=False, revision='main'\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a15d93-ddf3-41da-97ec-fb3beb7b8f14",
   "metadata": {},
   "source": [
    "## define some data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47679c4d-b16f-42d8-9f70-911da94e94dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numbers of tx sentences: 63650\n",
      "the numbers of not possible tokens: 16327\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from w3lib.html import remove_tags\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset_dir = '../FastChat/data_com'\n",
    "save_tokens_file = \"../FastChat/Eurp.pkl\"\n",
    "save_sentences_file = \"../FastChat/Eurp_sentences.pkl\"\n",
    "not_possible_tokens_file = \"../FastChat/not_possible_tokens\"\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    pattern = r'>\\s*(.*?)\\n'\n",
    "    # normalize unicode characters\n",
    "    s = unicode_to_ascii(s)\n",
    "    # remove the XML-tags\n",
    "    # s = remove_tags(s)\n",
    "    # add white space before !.?\n",
    "    # s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    # s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    # s = re.sub(r'\\s+', r' ', s)\n",
    "    s = re.findall(pattern,s)\n",
    "    # change to lower letter  没有改小写，改写不符合使用常理\n",
    "    # s = s.lower()\n",
    "    return s\n",
    "\n",
    "\n",
    "def cutted_data(cleaned, MIN_LENGTH=60, MAX_LENGTH=70):\n",
    "    cutted_lines = list()\n",
    "    for line in cleaned:\n",
    "        length = len(line.split())\n",
    "        if length > MIN_LENGTH and length < MAX_LENGTH:\n",
    "            line = [word for word in line.split()]\n",
    "            cutted_lines.append(' '.join(line))\n",
    "    return cutted_lines\n",
    "\n",
    "def save_clean_sentences(sentence, save_path):\n",
    "    pickle.dump(sentence, open(save_path, 'wb'))\n",
    "    print('Saved: %s' % save_path)\n",
    "\n",
    "def process(text_path):\n",
    "    fop = open(text_path, 'r', encoding='utf8')\n",
    "    raw_data = fop.read()\n",
    "    # sentences = raw_data.strip().split('\\n')\n",
    "    # raw_data_input = [normalize_string(data) for data in sentences]\n",
    "    sentences = normalize_string(raw_data)\n",
    "    raw_data_input = sentences\n",
    "    raw_data_input = cutted_data(raw_data_input)\n",
    "    fop.close()\n",
    "    return raw_data_input\n",
    "\n",
    "if os.path.exists(save_tokens_file) and os.path.exists(save_sentences_file):\n",
    "    with open(save_tokens_file, \"rb\") as file:\n",
    "        tx_tokens = pickle.load(file)\n",
    "    with open(save_sentences_file, \"rb\") as file:\n",
    "        sentences = pickle.load(file)\n",
    "else:     \n",
    "    files = os.listdir(dataset_dir)\n",
    "    sentences = []\n",
    "    for fn in tqdm(files):\n",
    "        if not fn.endswith('.txt'): continue\n",
    "        process_sentences = process(os.path.join(dataset_dir, fn))\n",
    "        sentences += process_sentences\n",
    "    print('the number of sentences:',len(sentences))\n",
    "    # remove the same sentences\n",
    "    a = {}\n",
    "    for aa in sentences:\n",
    "        if aa not in a:\n",
    "            a[aa] = 0\n",
    "        a[aa] += 1\n",
    "    sentences = list(a.keys())\n",
    "    print('Number of clean sentences: {}'.format(len(sentences)))\n",
    "    tx_tokens = []\n",
    "    for seq in tqdm(sentences):\n",
    "        tokens = tokenizer(seq,return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "        tx_tokens.append(tokens)\n",
    "    with open(save_tokens_file, 'wb') as f:\n",
    "        pickle.dump(tx_tokens, f)\n",
    "    with open(save_sentences_file, 'wb') as f:\n",
    "        pickle.dump(sentences, f)\n",
    "print('The numbers of tx sentences:',len(tx_tokens))\n",
    "# find not possible tokens\n",
    "if os.path.exists(not_possible_tokens_file):\n",
    "    with open(not_possible_tokens_file, \"rb\") as file:\n",
    "        not_possible_tokens = pickle.load(file)\n",
    "else:\n",
    "    all_tokens = set()\n",
    "    for token_list in tx_tokens:\n",
    "        all_tokens.update(token_list.tolist())    \n",
    "    all_possible_tokens = set(range(32000))\n",
    "    not_possible_tokens = all_possible_tokens - all_tokens\n",
    "    not_possible_tokens = list(not_possible_tokens)\n",
    "    not_possible_tokens.append(1)\n",
    "    with open(not_possible_tokens_file, 'wb') as f:\n",
    "        pickle.dump(not_possible_tokens, f)\n",
    "print('the numbers of not possible tokens:',len(not_possible_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656b0e6-e387-4914-8312-407ce8d1d5ae",
   "metadata": {},
   "source": [
    "## Define collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b2426d-e603-456d-bdf1-608a3e878f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ca13d7933d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def modulate_8QAM(bit_sequence):\n",
    "    if len(bit_sequence) % 3 != 0:\n",
    "        raise ValueError(\"The length of the bit sequence must be a multiple of 3.\")    \n",
    "    constellation_points = {\n",
    "        (0,0,0): 1+1j, \n",
    "        (0,0,1): -1+1j,\n",
    "        (0,1,0): -1-1j, \n",
    "        (0,1,1): 1-1j,\n",
    "        (1,0,0): 1+np.sqrt(3), \n",
    "        (1,0,1): 0+1j*(1+np.sqrt(3)), \n",
    "        (1,1,0): 0-1j*(1+np.sqrt(3)), \n",
    "        (1,1,1): -1-np.sqrt(3)\n",
    "    }\n",
    "    modulated_signal = []\n",
    "    \n",
    "    for i in range(0, len(bit_sequence), 3):\n",
    "        bits = tuple(bit_sequence[i:i+3])\n",
    "        modulated_signal.append(constellation_points[bits])\n",
    "    \n",
    "    return np.array(modulated_signal)\n",
    "bit_sequence = np.array([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,0,0,0,1,1,1,1,0,1,1,1,0])\n",
    "symbols = modulate_8QAM(bit_sequence)\n",
    "plt.scatter(symbols.real, symbols.imag, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3d975-12d2-4b98-96b0-563773e74baa",
   "metadata": {},
   "source": [
    "## define some communication function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c2a13a-db17-4796-a019-7b1bf0e911d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 32000/32000 [00:00<00:00, 51232.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32000, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "# 生成32000个星座图,对应32000个token\n",
    "from tqdm import tqdm\n",
    "from commpy.utilities import signal_power,dec2bitarray\n",
    "from commpy.channels import awgn,SISOFlatChannel\n",
    "import numpy as np\n",
    "from commpy.modulation import QAMModem\n",
    "M=8\n",
    "\n",
    "#  token列表转换为bit序列，N为位宽\n",
    "def token_to_bits(token: list, N: int):\n",
    "    numbytes=[]\n",
    "    [numbytes.extend(dec2bitarray(t,N).tolist()) for t in token]\n",
    "    return np.array(numbytes)\n",
    "\n",
    "# token转换为发射调制符号，\n",
    "def token_tx(token: list):\n",
    "  # 限制token值在0-32000范围内\\\n",
    "    vocab_size = 32000\n",
    "    token = [min(t, vocab_size) for t in token]  \n",
    "    # 将所有token转换为bit数组\n",
    "    bits = token_to_bits(token, 15)  \n",
    "    qam_symbols = modulate_8QAM(bits)\n",
    "    return qam_symbols\n",
    "\n",
    "# # 创建星座图\n",
    "def creat_constellation(M,vocab_size=32000):\n",
    "    # N = 2**(math.ceil(math.log2(math.ceil(math.log2(vocab_size)))))\n",
    "    constellation = np.zeros([vocab_size,math.ceil(15/np.log2(M))],dtype = np.complex64)\n",
    "    # print(token_tx([0],N).shape)\n",
    "    for i in tqdm(range(vocab_size)):\n",
    "        constellation[i,:] =  token_tx([i]) \n",
    "    return constellation\n",
    "\n",
    "\n",
    "constellation = creat_constellation(M)\n",
    "print(constellation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f169d18-7c8a-4904-9b0f-f13c21c242d4",
   "metadata": {},
   "source": [
    "## Define channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71335814-73c2-4f51-a0b4-c4e31a0b5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn, random, standard_normal\n",
    "class Channels():\n",
    "    def __init__(self,snr_db):\n",
    "        self.n_var = 4.70 / (10 ** (snr_db/ 10.0))\n",
    "        self.snr_db = snr_db\n",
    "    def AWGN(self,Tx_sig):\n",
    "        # dims = len(Tx_sig)\n",
    "        Rx_sig = [0] * len(Tx_sig)\n",
    "        dims = next((i for i, x in enumerate(Tx_sig) if x == 0), len(Tx_sig))\n",
    "        Rx_sig[:dims] = awgn(Tx_sig[:dims],self.snr_db)\n",
    "        return Rx_sig\n",
    "#     瑞丽信道,假设信道有完美的信道估计,所以此处返回H的值\n",
    "    def Rayleigh(self,Tx_sig):\n",
    "        Rx_sig = [0] * len(Tx_sig)\n",
    "        nb_symb = next((i for i, x in enumerate(Tx_sig) if x == 0), len(Tx_sig))\n",
    "        H = (standard_normal(nb_symb) + 1j * standard_normal(nb_symb)) * math.sqrt(0.5)\n",
    "        # print(H.shape)\n",
    "        Rx_sig[:nb_symb] = Tx_sig[:nb_symb] * H\n",
    "        Rx_sig = self.AWGN(Rx_sig)\n",
    "        Rx_sig[:nb_symb] = Rx_sig[:nb_symb]\n",
    "        return Rx_sig,H\n",
    "    def noise_std(self,):\n",
    "        return self.n_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb6fb87-caaa-44be-9d09-332588b9fdf1",
   "metadata": {},
   "source": [
    "## Rewrite 'generate' in GenerationMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c21e404-e637-492d-a625-00700030a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import CustomGenerationMixin\n",
    "model.generate = CustomGenerationMixin.generate.__get__(model)\n",
    "model._beam_search = CustomGenerationMixin._beam_search.__get__(model)\n",
    "model.euclidean_distance_squared = CustomGenerationMixin.euclidean_distance_squared.__get__(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689fefb0-cf9d-4886-b5a3-4f39c5dcbeec",
   "metadata": {},
   "source": [
    "## some test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "287b4b7f-c411-4a14-8eb8-123b97a931e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 84])\n",
      "received_symbols shape: (2, 395)\n",
      "(2, 395)\n",
      "tx power: 3.816978087541011\n",
      "bleu: [0.8615384615384616, 0.9838709677419355]\n",
      "used time: 6.744422435760498\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "start = time.time()\n",
    "batch_size = 2\n",
    "tx_tokens = tx_tokens[1:]\n",
    "batch_sentences = tx_tokens[:batch_size]\n",
    "padded_batch = pad_sequence(batch_sentences, batch_first=True)\n",
    "print(padded_batch.shape)\n",
    "start_ids = 5\n",
    "lengths = torch.tensor([len(sentence) for sentence in batch_sentences])\n",
    "transmitted_symbols = np.zeros([batch_size,(padded_batch.shape[1]-start_ids)*constellation.shape[1]],dtype = np.complex64)\n",
    "received_symbols = np.zeros([batch_size,(padded_batch.shape[1]-start_ids)*constellation.shape[1]],dtype = np.complex64)\n",
    "H = np.ones_like(received_symbols,dtype=np.complex64)\n",
    "print('received_symbols shape:',received_symbols.shape)\n",
    "M=8\n",
    "N=15\n",
    "snr_db = 6\n",
    "channel =Channels(snr_db)\n",
    "N0 = channel.noise_std()#/math.sqrt(0.5)\n",
    "# print(padded_batch[1,1:])\n",
    "for i in range(padded_batch.shape[0]):\n",
    "    transmitted_symbols[i,0:(tx_tokens[i].shape[-1]-start_ids)*constellation.shape[1]] = token_tx(np.array(tx_tokens[i][start_ids:]))\n",
    "    received_symbols[i,0:(tx_tokens[i].shape[-1]-start_ids)*constellation.shape[1]],H[i,0:(tx_tokens[i].shape[-1]-start_ids)*constellation.shape[1]] = channel.Rayleigh(transmitted_symbols[i,0:(tx_tokens[i].shape[-1]-start_ids)*constellation.shape[1]])\n",
    "print(transmitted_symbols.shape)\n",
    "power = signal_power(transmitted_symbols)\n",
    "print('tx power:',power)\n",
    "\n",
    "num_beams = 10\n",
    "out_put = model.generate(padded_batch[:,0:start_ids].cuda(),received_symbols,constellation,M,N,N0,H,not_possible_tokens,num_beams = num_beams,do_sample = False, max_new_tokens = padded_batch.shape[1])\n",
    "# # \n",
    "bleu_score = []\n",
    "for i in range(out_put.shape[0]):\n",
    "    rx_str = tokenizer.decode(out_put[i],\n",
    "        skip_special_tokens=True,\n",
    "        spaces_between_special_tokens=False,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "        padding_side='left')\n",
    "    tx_str = tokenizer.decode(tx_tokens[i],\n",
    "        skip_special_tokens=True,\n",
    "        spaces_between_special_tokens=False,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "        padding_side='left')\n",
    "    bleu_score.append(sentence_bleu([rx_str[0:len(tx_str)].split()],tx_str.split(),weights=(1,0,0,0)))\n",
    "print('bleu:',bleu_score)\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print('used time:',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e62fcb9d-6bcc-4bb3-a529-5ca34cbf2b49",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../FastChat/data_com/Eurp.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m  \n\u001b[1;32m     35\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m---> 36\u001b[0m test_eur \u001b[38;5;241m=\u001b[39m \u001b[43mEurData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m power \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14.2470\u001b[39m\n\u001b[1;32m     38\u001b[0m smoothing_function \u001b[38;5;241m=\u001b[39m SmoothingFunction()\u001b[38;5;241m.\u001b[39mmethod1\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mEurData.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      7\u001b[0m     data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../FastChat/data_com/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEurp.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m40\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/fastchat/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../FastChat/data_com/Eurp.pkl'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29992fe4-9b59-4f25-89cb-31969c761fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
